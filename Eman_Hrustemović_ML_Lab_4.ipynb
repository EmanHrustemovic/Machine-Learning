{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## [Lab 4] Linear Regression"
      ],
      "metadata": {
        "id": "oWBzRWBYqrzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Linear regression is a fundamental statistical method used to model the relationship between a dependent variable and one or more independent variables. It assumes a linear relationship between the variables, allowing us to predict the value of the dependent variable based on the known values of the independent variables.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "The goal of this lab session is focused on learning linear regression and interpreting its models. We will work with the real-world dataset, apply techniques to fit lines to data points, assess model performance, understand assumptions, and interpret results.\n",
        "\n",
        "---\n",
        " Some of the useful and most important equations:\n",
        "\n",
        "\n",
        "> Linear Regression Hypothesis : $h(x) = w_{0}+ w_{1}x$\n",
        "\n",
        "\n",
        "\n",
        "> Linear Regression Model : $y(x) = 1 + 1\\cdot x$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> First Partial Derivatives :\n",
        "> $ \\frac{\\partial J}{\\partial w_{0}} = \\frac{1}{m} \\sum_{i=1}^{m} (y'^{(i)}-y^{(i)}) $,\n",
        "> $ \\frac{\\partial J}{\\partial w_{1}} = \\frac{1}{m} \\sum_{i=1}^{m} (y'^{(i)}-y^{(i)}) \\cdot x^{(i)} $\n",
        "\n"
      ],
      "metadata": {
        "id": "jqysWDO7qwj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> For the purpose of this lab, we will use [this](https://drive.google.com/file/d/104Cz-K9qiZ0FWnanl09gUULn58M8vau1/view?usp=share_link) dataset. You can download it from the LMS or from the Google Drive link, and upload to your notebook.\n",
        "\n",
        "> It only contains two columns: house price ($ y $) and size of the house ($ x $).\n"
      ],
      "metadata": {
        "id": "vxVd873LZwtK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.   Read and Write operations on a file\n",
        "In order to analyze and work with our dataset, firstly, we need to read its content. You can use the following code snippet. Additionally, you can use the helper *is_number* function, to handle possible non-numeric values in the columns."
      ],
      "metadata": {
        "id": "5mLV_-7NdrGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_number(s):\n",
        "    if s is None: return False\n",
        "    try:\n",
        "        complex(s)\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "def read_file(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            data = []\n",
        "            lines = file.readlines()\n",
        "\n",
        "            keys = lines[0].split(',')\n",
        "            for i in range(1, len(lines)):\n",
        "                data_row = [\n",
        "                    float(cell.strip('\\n')) if is_number(cell.strip('\\n')) else cell.strip('\\n')\n",
        "                    for cell in lines[i].split(',')\n",
        "                ]\n",
        "                data.append(data_row)\n",
        "            return data\n",
        "    except FileNotFoundError:\n",
        "        raise RuntimeError(\"File not found.\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "doa2c0EFYjbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path=r'C:Desktop/Machine Learning/house_price.csv'\n",
        "with open(file_path,'r') as file:\n",
        "  data=[]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "WMcq16JYW4vB",
        "outputId": "8a4d7611-10dd-438a-8f13-e9ef42015126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'C:Desktop/Machine Learning/house_price.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-98dc132fed67>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mr'C:Desktop/Machine Learning/house_price.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:Desktop/Machine Learning/house_price.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Linear Regression Cost Function\n",
        "\n",
        "> This function should compute the cost function for a given set of parameters $w_{0}, w_{1}$\n",
        "(intercept and slope) of a linear regression model, based on the Mean Squared Error (MSE) formula. It iterates over each data point, calculates the predicted value using the current parameters, and accumulates the squared errors. Finally, it divides the sum by $2m$ (the number of data points), to get the average squared error. This function evaluates how well the linear regression model fits the training data.\n",
        "\n",
        "> The equation for the cost function is:\n",
        "\n",
        "> $J(w_{0}, w_{1}) = \\frac{1}{2m} \\sum_{i=1}^{m} (y'^{(i)}-y^{(i)})^2 $, where $y'^{(i)} = h(x^{(i)})$"
      ],
      "metadata": {
        "id": "Gmk9olvBdzZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_reg_cost(w0, w1, x, y):\n",
        "  pass"
      ],
      "metadata": {
        "id": "hQFf3qiueaQx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_reg_cost(w0,w1,x,y):\n",
        "  m=len(x)\n",
        "\n",
        "  errors=0\n",
        "\n",
        "  for i in range(m):\n",
        "    y_predicted=w0+w1*x[i]\n",
        "    squared_errors=(y_predicted-y[i])**2\n",
        "    errors+=squared_errors\n",
        "\n",
        "  MSE=(1/(2*m))**errors\n",
        "  return MSE\n",
        "\n"
      ],
      "metadata": {
        "id": "jkCEtYP-xv-R"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w0 = 0.1\n",
        "w1 = 0.2\n",
        "\n",
        "x = [1, 2, 3, 4, 5]\n",
        "y = [1.5, 2.5, 3.5, 4.5, 5.5]\n",
        "\n",
        "\n",
        "print(linear_reg_cost(w0,w1,x,y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TEftENgymf0",
        "outputId": "e43577fc-3b95-43fb-da54-13341c4ea905"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Linear Regression Delta Cost\n",
        "\n",
        "> This function should compute the gradients of the cost function with respect to the parameters $(w_{0}, w_{1})$.  These gradients represent the direction and magnitude of adjustment needed to minimize the cost function, facilitating optimization techniques like gradient descent. By accumulating errors weighted by the corresponding features, it calculates the partial derivatives of the cost function and then averages these values by dividing by the number of data points $m$."
      ],
      "metadata": {
        "id": "eUlpVwSFmL-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_reg_delta_cost(w0, w1, x, y):\n",
        "  pass"
      ],
      "metadata": {
        "id": "kiLr6r3ze0F6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_reg_delta_cost(w0,w1,x,y):\n",
        "  sumY=0\n",
        "  sumX=0\n",
        "\n",
        "  m=len(x)\n",
        "\n",
        "  for i in range(m):\n",
        "    y_predicted=w0+w1*x[i]\n",
        "    difference=y_predicted -y[i]\n",
        "\n",
        "    sumY+=difference\n",
        "    sumX+=difference*x[i]\n",
        "\n",
        "  derivationX=sumX/m\n",
        "  derivationY=sumY/m\n",
        "\n",
        "  return derivationX,derivationY\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jwJScSrkzWEP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w0 = 0.1\n",
        "w1 = 0.2\n",
        "\n",
        "x = [1, 2, 3, 4, 5]\n",
        "y = [1.5, 2.5, 3.5, 4.5, 5.5]\n",
        "\n",
        "print(linear_reg_delta_cost(w0,w1,x,y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSLV_3qU0i3u",
        "outputId": "37c56814-066d-4889-a187-2400d6c038c3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(-10.0, -2.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. Training the Linear Regression Model\n",
        "\n",
        "> To train the model, we are adjusting the slope and intercept values.\n",
        "\n",
        "> \\begin{align*}\n",
        "    w_0 &:= w_0 - \\frac{\\partial}{\\partial w_0} J(w_0, w_1) \\\\\n",
        "    w_1 &:= w_1 - \\frac{\\partial}{\\partial w_1} J(w_0, w_1)\n",
        "\\end{align*}\n",
        "\n",
        "The adjustment of weights is a crucial step in the optimization process. It occurs through gradient descent which measures the disparity between predicted and actual values. By iteratively updating the weights in the direction opposite to the gradient of the cost function, the algorithm seeks to descend towards the minimum of the cost function, thereby refining the model's parameters to better fit the data.\n"
      ],
      "metadata": {
        "id": "8_1FnlCTe1Q1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_linear_reg(x,\n",
        "                     y,\n",
        "                     w0 = 0,\n",
        "                     w1 = 0,\n",
        "                     learning_rate = 0.0000004,\n",
        "                     num_iterations = 1000,\n",
        "                     logging = False):\n",
        "  pass"
      ],
      "metadata": {
        "id": "_3WnN-2le1Yv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_linear_reg(x,y,w0,w1,learning_rate,num_iterations,logging=False):\n",
        "  m=len(x)\n",
        "  costs=[]\n",
        "\n",
        "  for i in range(num_iterations):\n",
        "    y_predicted=[w0+w1*xi for xi in x]\n",
        "\n",
        "\n",
        "    gradientW0=(1/m)*sum([(y_predicted[j]-yi) for j, yi in enumerate(y)])\n",
        "    gradientW1=(1/m)*sum([(y_predicted[j]-yi) * xi for j, (xi,yi) in enumerate(zip(x,y))])\n",
        "\n",
        "    w0 -= learning_rate * gradientW0\n",
        "    w1 -= learning_rate * gradientW1\n",
        "\n",
        "    cost=linear_reg_cost(w0,w1,x,y)\n",
        "    costs.append(cost)\n",
        "    if logging:\n",
        "      print(f\"Iteration {i+1}: Cost = {cost}\")\n",
        "\n",
        "  return w0, w1, costs\n"
      ],
      "metadata": {
        "id": "B6D_FAKz1Tw8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w0 = 0.1\n",
        "w1 = 0.2\n",
        "\n",
        "x = [1, 2, 3, 4, 5]\n",
        "y = [1.5, 2.5, 3.5, 4.5, 5.5]\n",
        "\n",
        "learning_rate=0.0000000004\n",
        "num_iterations=1000\n",
        "logging=False\n",
        "\n",
        "print(train_linear_reg(x,y,w0,w1,learning_rate,num_iterations,logging))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xix-38vd1TkP",
        "outputId": "b7f857b1-bdbc-4d5c-ef6e-0ec404fea8c2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.10000111999737864, 0.20000399999053745, [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, you will adjust these parameters according to your preference."
      ],
      "metadata": {
        "id": "8dK1LedMc5sx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.4. Visualization\n",
        "> You can try to plot and visualize the regression line alongside the data points, to assess the model's fit and to see its performance when the adjustment of parameters is done. Below, you can find a helper column function, that will extracts a specific column. In our case, it is used to find the values of dependant and independant variable from our dataset."
      ],
      "metadata": {
        "id": "hAjEBQa4OzU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def column(matrix, i):\n",
        "    return [row[i] for row in matrix]"
      ],
      "metadata": {
        "id": "a8EeSU3ve-mk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matrix=[[2,4,5],[3,7,10],[70,80,45]]\n",
        "i=1\n",
        "print(column(matrix,i))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-GTb_MU5oqq",
        "outputId": "02e944fd-ff97-41b3-dc62-4d3cce345fe7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4, 7, 80]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "yzynG1cogCNv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Vectorization\n",
        "\n",
        "\n",
        "> Vectorization is a technique that performs operations on entire arrays or matrices in a single operation rather than looping through each element individually. Now, your task is to implement and to train the linear regression model, by using the vectorization. Your method skeleton will look a bit different now and you will need to rewrite them, by using the matrix methods covered last week.\n",
        "\n"
      ],
      "metadata": {
        "id": "t2cVxS68lkqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1. Linear Regression Cost Function\n",
        "> This function takes three matrices as the parameters:\n",
        "\n",
        "\n",
        "*   $ w $ (vector of weights, that contains slope and intercept for the independent variable)\n",
        "*  $ x $ (matrix of input features - each row of $x$ corresponds to a single entry data point)\n",
        "*  $ y $ (vector of each target value --> dependant variables)\n",
        "\n",
        "> By using the matrix methods that we implemented last week, implement this cost function.\n",
        "\n",
        "> **A bit of a tip**: To calculate the predicted values, use the matrix multiplication. To calculate the difference between actual and predicted values, use the diff method. The only thing left is to compute the square difference and divide it with the number of data points doubled."
      ],
      "metadata": {
        "id": "yrbKDuGwQjuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_reg_cost(w, x, y):\n",
        "  pass"
      ],
      "metadata": {
        "id": "vlD8C-5pQj_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_reg_cost(w,x,y):\n",
        "  m=len(y)\n",
        "  sum_squared_diff=0\n",
        "\n",
        "  for i in range(m):\n",
        "    y_predicted_i=w[0]+w[1]*x[i]\n",
        "    difference_i=(y_predicted_i-y[i])**2\n",
        "    sum_squared_diff+=difference_i\n",
        "\n",
        "  cost=sum_squared_diff/(2*m)\n",
        "  return cost\n"
      ],
      "metadata": {
        "id": "RssV974y6zpH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w= [1,2]\n",
        "x = [1, 2, 3, 4, 5,6]\n",
        "y = [2,3,4,5,6]\n",
        "\n",
        "print(linear_reg_cost(w,x,y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxq-5b-QboBR",
        "outputId": "5991388d-7ff3-41ab-f7fd-2cf330d5104b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2. Linear Regression Delta Cost Function\n",
        "> To calculate the vectorized gradient, **here is a hint**. Firstly compute the predicted values and the difference between the actual and predicted values. Next, transpose the input features ($x$) and perform matrix multiplication to calculate the weighted sum of feature errors. At the end, scale the result by the inverse of the number of data points ($m$), to obtain the average gradient."
      ],
      "metadata": {
        "id": "1e-g0SfcS9nC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_reg_delta_cost(w, x, y):\n",
        "  pass"
      ],
      "metadata": {
        "id": "4aJVp0VETDkv"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_reg_delta_cost(w, x, y):\n",
        "    m = len(y)\n",
        "    sum_elements = [0] * len(w)\n",
        "\n",
        "    for i in range(m):\n",
        "        y_predicted = sum(w[j] * x[i][j] for j in range(len(w)))\n",
        "\n",
        "        difference = y_predicted - y[i]\n",
        "\n",
        "        for j in range(len(w)):\n",
        "            sum_elements[j] += difference * x[i][j]\n",
        "\n",
        "    gradient = [sum_elements[j] / m for j in range(len(w))]\n",
        "    return gradient\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dEmux_-AcjuT"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = [0.1, 0.2]\n",
        "x = [[1, 2], [1, 3], [1, 5]]\n",
        "y = [2.3, 4.5, 6.2]\n",
        "\n",
        "gradient = linear_reg_delta_cost(w, x, y)\n",
        "print(\"Our calculated gradient is:\", gradient)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRkBgHWJeWz3",
        "outputId": "4e6c591e-ea39-4b04-a657-727c508ae46e"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our calculated gradient is: [-3.5666666666666664, -13.5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3. Training the Linear Regression Model\n",
        "\n",
        "> As we mentioned above, the algorithm continuously adjusts the weights in a manner that moves them away from the direction where the cost function increases the most, aiming to approach the minimum of the cost function. This iterative process enables the algorithm to gradually refine the model's parameters, enhancing its ability to accurately capture the patterns in the data. The logic for this function stays the same as for the initial one; you will just need to adjust your code, to use the matrix methods, instead of writing the loops and iterating through elements. The function should return the optimized parameter $ w$ and the final cost."
      ],
      "metadata": {
        "id": "ljMSFYr8VMay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_linear_reg(x,\n",
        "                     y,\n",
        "                     w,\n",
        "                     learning_rate = 0.0000004,\n",
        "                     num_iterations = 1000,\n",
        "                     logging = True):\n",
        "  pass"
      ],
      "metadata": {
        "id": "A2Ck2zSCVbP-"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_linear_reg(x, y, w, learning_rate, num_iterations, logging=False):\n",
        "    m = len(x)\n",
        "\n",
        "\n",
        "    cost_history = []\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        y_pred = [w[0] + w[1] * xi for xi in x]\n",
        "        errors = [y_pred[j] - y[j] for j in range(m)]\n",
        "        grad_w0 = sum(errors) / m\n",
        "        grad_w1 = sum([errors[j] * x[j] for j in range(m)]) / m\n",
        "\n",
        "        w[0] -= learning_rate * grad_w0\n",
        "        w[1] -= learning_rate * grad_w1\n",
        "\n",
        "\n",
        "        cost = sum([(errors[j] ** 2) for j in range(m)]) / (2 * m)\n",
        "\n",
        "\n",
        "        cost_history.append(cost)\n",
        "        if logging and (i + 1) % 10 == 0:\n",
        "            print(\"Iteration {}, Cost: {}\".format(i + 1, cost))\n",
        "\n",
        "\n",
        "    return w, cost_history[-1]\n"
      ],
      "metadata": {
        "id": "Nx-pkUX3eVae"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = [1, 2, 3, 4, 5]\n",
        "y_train = [2, 4, 5, 4, 5]\n",
        "\n",
        "initial_w = [0, 0]\n",
        "learning_rate = 0.01\n",
        "num_iterations = 100\n",
        "\n",
        "optimized_w, final_cost = train_linear_reg(x_train, y_train, initial_w, learning_rate, num_iterations, logging=True)\n",
        "\n",
        "\n",
        "print(\"Optimized weights:\", optimized_w)\n",
        "print(\"Final cost:\", final_cost)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pz-SlAlgjDHn",
        "outputId": "79b09354-ec6d-4064-e55d-8b6455f4097a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 10, Cost: 1.3880624884453763\n",
            "Iteration 20, Cost: 0.611637419309193\n",
            "Iteration 30, Cost: 0.5397728346866021\n",
            "Iteration 40, Cost: 0.5250032359733958\n",
            "Iteration 50, Cost: 0.515134065543037\n",
            "Iteration 60, Cost: 0.5059486199257444\n",
            "Iteration 70, Cost: 0.49709743451512345\n",
            "Iteration 80, Cost: 0.4885430552128612\n",
            "Iteration 90, Cost: 0.48027348430514943\n",
            "Iteration 100, Cost: 0.4722790745563475\n",
            "Optimized weights: [0.6051164658853391, 1.0417530517794173]\n",
            "Final cost: 0.4722790745563475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.4. Visualization & Plotting\n",
        "> Additionally, you can try to plot and visualize the regression line alongside the data points."
      ],
      "metadata": {
        "id": "jzWXtm5SVhEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "6rhyoHAjWaDs"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x_train, y_train, color='blue', label='Original data')\n",
        "\n",
        "predicted_y = [optimized_w[0] + optimized_w[1] * x for x in x_train]\n",
        "plt.plot(x_train, predicted_y, color='red', label='Regression line')\n",
        "\n",
        "\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Linear Regression')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "ydI2DFLXlAgQ",
        "outputId": "34ab1643-84b7-481a-c1da-af1ee9da0a15"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}